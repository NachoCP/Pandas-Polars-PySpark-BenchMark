{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory /Users/NachoCorcuera/PycharmProjects/Pandas-Polars-PySpark-BenchMark/notebooks\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    " \n",
    "import os\n",
    "\n",
    "\n",
    "# Start timer\n",
    "\n",
    "# get current directory\n",
    "path = os.getcwd()\n",
    "print(\"Current Directory\", path)\n",
    " \n",
    "# prints parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(path, os.pardir))\n",
    "input_dir = f\"{parent_dir}/data\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polars Lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def join_df():\n",
    "    regions = [\n",
    "        \"Europe\", \n",
    "        \"North America\", \n",
    "        \"Asia\", \n",
    "        \"Sub-Saharan Africa\", \n",
    "        \"Central America and the Caribbean\", \n",
    "        \"Middle East and North Africa\", \n",
    "        \"Australia and Oceania\"\n",
    "    ]\n",
    "\n",
    "    aliases = [\n",
    "        \"EU\", \n",
    "        \"NA\", \n",
    "        \"AS\", \n",
    "        \"SSA\", \n",
    "        \"CA\", \n",
    "        \"MENA\", \n",
    "        \"AUS\"\n",
    "    ]\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df_regions = pl.DataFrame({\n",
    "        \"Region\": regions,\n",
    "        \"Alias\": aliases\n",
    "    }).lazy()\n",
    "\n",
    "    # Display the DataFrame to verify its contents\n",
    "    return df_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "def processing_df(file):\n",
    "    times = {}\n",
    "    start_time_full = time.time()\n",
    "\n",
    "    ## Reading time\n",
    "    df = pl.scan_csv(f\"{input_dir}/{file}.csv\")\n",
    "    end_time = time.time()\n",
    "    times[\"read_csv\"] = end_time - start_time_full\n",
    "\n",
    "    ## Filtering time\n",
    "    start_time = time.time()\n",
    "    filtered_polars = df.filter(pl.col('Total Profit') > 2000)\n",
    "    end_time = time.time()\n",
    "    times[\"filter\"] = end_time - start_time\n",
    "\n",
    "    ## Aggregation\n",
    "    start_time = time.time()\n",
    "    df.group_by('Region').agg(pl.col('Total Profit').sum().alias('sales'),\n",
    "                            pl.col('Total Profit').mean().alias('sales_mean'),\n",
    "                            pl.col('Total Profit').max().alias('sales_max'),\n",
    "                            pl.col('Total Profit').min().alias('sales_min'),\n",
    "                            pl.col('Total Profit').median().alias('sales_median'))\n",
    "    end_time = time.time()\n",
    "    times[\"aggregation\"] = end_time - start_time\n",
    "\n",
    "\n",
    "    ## Joining time\n",
    "    start_time = time.time()\n",
    "    df_regions = join_df()\n",
    "    df_joined = df.join(df_regions, on=\"Region\", how=\"left\")\n",
    "    end_time = time.time()\n",
    "    times[\"join\"] = end_time - start_time\n",
    "\n",
    "\n",
    "    ## Writting time\n",
    "    start_time = time.time()\n",
    "    df_regions = join_df()\n",
    "    df_joined.collect(streaming=True).write_csv(\"testing_write_2.csv\")\n",
    "    end_time = time.time()\n",
    "    times[\"write\"] = end_time - start_time\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [\"sales_50000\",\n",
    "             \"sales_250000\",\n",
    "             \"sales_1000000\",\n",
    "             \"sales_5000000\",\n",
    "             \"sales_25000000\"]\n",
    "\n",
    "\n",
    "times = {file: processing_df(file) for file in file_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import psutil\n",
    "def processing_df_parquet(file):\n",
    "    times = {}\n",
    "    start_time_full = time.time()\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_before = process.memory_info().rss / 1024 ** 2  # Convert bytes to MB\n",
    "    cpu_before = process.cpu_percent(interval=None)\n",
    "    ## Reading time\n",
    "    df = pl.scan_parquet(f\"{input_dir}/{file}/*.parquet\")\n",
    "    end_time = time.time()\n",
    "    times[\"read_csv\"] = end_time - start_time_full\n",
    "\n",
    "    ## Filtering time\n",
    "    start_time = time.time()\n",
    "    filtered_polars = df.filter(pl.col('Total Profit') > 2000)\n",
    "    end_time = time.time()\n",
    "    times[\"filter\"] = end_time - start_time\n",
    "\n",
    "    ## Aggregation\n",
    "    start_time = time.time()\n",
    "    df.group_by('Region').agg(pl.col('Total Profit').sum().alias('sales'),\n",
    "                            pl.col('Total Profit').mean().alias('sales_mean'),\n",
    "                            pl.col('Total Profit').max().alias('sales_max'),\n",
    "                            pl.col('Total Profit').min().alias('sales_min'),\n",
    "                            pl.col('Total Profit').median().alias('sales_median'))\n",
    "    end_time = time.time()\n",
    "    times[\"aggregation\"] = end_time - start_time\n",
    "\n",
    "\n",
    "    ## Joining time\n",
    "    start_time = time.time()\n",
    "    df_regions = join_df()\n",
    "    df_joined = df.join(df_regions, on=\"Region\", how=\"left\")\n",
    "    end_time = time.time()\n",
    "    times[\"join\"] = end_time - start_time\n",
    "\n",
    "\n",
    "    ## Writting time\n",
    "    start_time = time.time()\n",
    "    df_regions = join_df()\n",
    "    df_joined.collect(streaming=True).write_parquet(\"testing_write_2\")\n",
    "    end_time = time.time()\n",
    "    times[\"write\"] = end_time - start_time\n",
    "    \n",
    "    print(psutil.cpu_percent())\n",
    "    print(psutil.virtual_memory())  # physical memory usage\n",
    "    print('memory % used:', psutil.virtual_memory()[2])\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.2\n",
      "svmem(total=34359738368, available=13390675968, percent=61.0, used=5322358784, free=6151897088, active=2801582080, inactive=6037389312, wired=2520776704)\n",
      "memory % used: 61.0\n"
     ]
    }
   ],
   "source": [
    "file = \"parquet/sales_25000000\"\n",
    "x = processing_df_parquet(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
